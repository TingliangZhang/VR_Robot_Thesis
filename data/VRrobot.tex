\chapter{项目背景}

随着机器人技术的发展，机器人的应用场景越来越多。但是目前自动控制机器人尚不能执行多数复杂任务，特别是抢险救灾等需要随机应变的任务，这种情况下需要遥操作机器人。目前机器人远程控制普遍采用手柄或键盘控制方式，且监控方式普遍为摄像头图像显示在监视器上，与现场操作差别很大。


\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{CtrlPanel.jpg}
\caption{控制面板} 
\label{fig:xfig1}
\end{figure}

现有的基于动作捕捉的遥操作系统如OptiTrack和机械外骨骼等，虽然用户体验很好，但是成本极高，依赖性强，无法大规模应用；现有的实时VR显示大多基于图像拼接，图像采集设备昂贵，实时性和图像质量无法同时兼顾，且对算力要求很高。

本作品提出了一种廉价、低延时的遥操作系统，其双目摄像头及VR显示系统可使操作者看到具有立体感的实时画面，手持追踪器操作机械臂末端符合人类日常使用手进行操作的习惯，可以给予操作者以身临其境的操作体验。


\chapter{现有系统问题}

\section{现有的三维图像采集系统}

目前三维场景重构需要很高的算力，分辨率低且不能保证实时性，目前较成熟的实时三维场景采集系统基本上都基于图像拼接技术：

\begin{figure}
\begin{minipage}{0.48\textwidth}
  \centering
  \includegraphics[height=4cm]{Cube.jpg}
  \caption{立方体拼接}
  \label{fig:p1}
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
  \centering
  \includegraphics[height=4cm]{Tri.jpg}
  \caption{金字塔形拼接}
  \label{fig:p2}
\end{minipage}
\end{figure}

就图像传输系统而言，要创建一个360度的视频，无论是使用一组特殊的摄像机同时记录360度的场景，还是将4个不同角度的GoPros图像拼接在一起。传入的360度视频文件是4K和更高，比特率可以超过50 Mb/s。而用于VR的3D 360度视频是其两倍，即每小时44 GB视频。如果要实时发送和接收这些图像信息，对带宽等网络环境要求非常高。另外无论采用立方体还是金字塔形拼接方式，拼接这些图像会消耗大量的算力，运算和传输延迟加起来都会达到2s以上，实时性和图像质量无法同时兼顾。

就图像采集设备而言，以Google Jump为例，Google Jump使用24个GoPro，环绕一个圆形分布，考虑到GoPro单个相机的价格（2000-3000人民币）上述系统造价非常昂贵。
\begin{figure}[htbp]
\small
\centering
\includegraphics[width=16cm]{google-jump.jpg}
\caption{Google Jump} 
\end{figure}

\section{现有的机器人遥操作系统}
\subsection{计算机指令或手柄控制}
许多需要人工远程操控的机器人，其操作方式基本都是向控制面板或计算机输入指令而实现。工作人员需要熟悉各种操作平台，并需要长时间紧盯电脑屏幕进行操作，其交互体验不是很理想。

另一些用户界面比较友好的机器人使用摇杆控制

\subsection{OptiTrack}
OptiTrack\footnote{\url{http://optitrack.com/}} 是基于一系列红外摄像机的动作捕捉系统解决方案，在操作者身上贴多个感光球，通过四周的十多个红外摄像机来计算每一个感光球的位置，从而获得人的动作数据。
\begin{figure}[H]
\small
\centering
\includegraphics[width=16cm]{movementPrime41_16.png}
\caption{OptiTrack} 
\end{figure}
由于此解决方案需要至少12个高速红外摄像机和配套的处理系统，而一个红外摄像机售价4000-5000美元，整个系统的成本非常高，用其做机器人控制并不现实。

\subsection{T-HR3}

T-HR3是一种远程机动系统，它通过外骨骼和数据手套将手，手臂和脚部动作映射到机器人。

就操作者近端方面而言，T-HR3采用机械外骨骼加上双手的数据手套。由于机械外骨骼以力信号作为控制端的输出信号，所以，机械外骨骼和操作者之间本身就存在一定的滞后性，而且，力信号易受外力干扰，所以存在信号不稳定的问题。

另外，T-HR3需要一套昂贵且笨重的外骨骼，其应用场景十分受限。

\begin{figure}[htbp]
\small
\centering
\includegraphics[width=16cm]{T-HR3_2.jpg}
\caption{T-HR3} 
\end{figure}

\chapter{系统简介}

该项目可分为两个子系统：图像显示系统及机器人控制系统。
	
	\section{VR远程监视系统}
本项目旨在提供一个具有较强立体感的实时全景图像，并解决现有系统的问题以及最大化的降低造价，以使远程操控者获得身临其境的体验。我们用ZED双目摄像头来采集实时场景信息，两个目采集到的图像对应到VR眼镜的两个显示屏中，而搭载ZED的多自由度舵机支架可随操纵者头部的转动而实时旋转，以保证双目摄像头的朝向和操作者双眼朝向一致，从而使操作者可以实时地看到各方位的立体场景。
	
此系统硬件部分仅由一个双目摄像头，转台，虚拟现实头盔组成，成本低，且部件较常见。双目摄像头可以由两个独立的摄像头替代，只需保证两摄像头间距和瞳距相似即可，随着虚拟现实技术的发展，分辨率较高的Vive VR头盔(见图~\ref{vivevr})价格可以为普通人所接受，Google Cardboard(见图~\ref{card})售价低至10RMB，和智能手机配合使用。
\begin{figure}
\begin{minipage}{0.48\textwidth}
  \centering
  \includegraphics[height=4cm]{HMD.jpg}
  \caption{Vive VR头盔}
  \label{vivevr}
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
  \centering
  \includegraphics[height=4cm]{Google-Cardboard.jpg}
  \caption{Google Cardboard}
  \label{card}
\end{minipage}
\end{figure}

因为只需要传输两个摄像头的图像，且并不需要复杂的运算，这套机械式的实时全景系统延迟很小。

\section{Vive追踪器遥操作系统}

虚拟现实头盔和追踪器上的红外定位模块精确的确定了它们的绝对位置，由此可得它们的相对位置，我们可以使机械臂末端和双目摄像头的相对位置和其一致，从而实现操作者直接用手的位置来控制机械臂。

此系统使得操作者能够利用自身的动作控制机器人的运动，符合人类的行为习惯，因而会带来良好的操作体验。

Vive虚拟现实套件\footnote{\url{https://www.vive.com/cn/product/}}(见图~\ref{vivekit})包括Vive追踪器，Vive VR头盔和Lighting基站，官网售价5488RMB，远低于OptiTrack或T-HR3外骨骼部分的售价。而这套系统完全可以胜任大多数任务。而且Vive VR套件对环境要求很低，可以随时搭建操作环境。
\begin{figure}[H]
\small
\centering
\includegraphics[width=16cm]{vive.PNG}
\caption{Vive虚拟现实套件} 
\label{vivekit}
\end{figure}

\chapter{技术细节}

\section{实时图像采集与显示系统}
项目用于和Vive头盔进行互动的是一个ZED双目摄像头。ZED双目摄像头搭载在由两个数字舵机组成的双自由度平台上，用于与用户交互。

整个系统放置在4轮全向轮底盘上面， ZED双目摄像头采集到的图像，通过USB3.0发送给Nvidia Jetson TX1\footnote{\url{http://www.nvidia.cn/object/embedded-systems-dev-kits-modules-cn.html}}，TX1上运行Ubuntu，图像在TX1上处理后，使用UV4L服务（User space Video4Linux）\footnote{\url{https://www.linux-projects.org/uv4l/}}发送到局域网。同时，Vive VR头盔的姿态数据通过HTC串流盒发送到计算机中进行处理，处理后通过蓝牙串口发送到全向轮车上面的Arduino Mega 2560，控制双自由度数字舵机做出与Vive VR头盔同步的转动，使安装在双自由度数字舵机平台上的ZED双目摄像头的指向与Vive头盔实时相同。

演示视频参见附件XXX

\section{机械臂控制系统}

\begin{figure}[htbp]
\small
\centering
\includegraphics[width=8cm]{Dobot0.png}
\caption{DOBOT机械臂及通讯系统} 
\end{figure}


\section{NAO机器人控制系统}
项目中为了控制机器人的手臂，采用逆运动学的理论，通过机器人手臂的端点结合手臂的参数来计算关节必要的旋转值来进行动作。由于NAO机器人手臂（不包括手指部分）的自由度是5个，因此在仅要设置手指尖端的目标坐标点的情况下，会出现手臂姿态不唯一的情况。通过筛选使得手臂各个关节与端点的坐标实现一一对应。

\begin{figure}[htbp]
\small
\centering
\includegraphics[width=8cm]{NAO.jpg}
\caption{NAO机器人} 
\end{figure}

对于机器人的姿态设定、手臂末端的设定涉及到机器人的控制理论。每个关节有相应的标准坐标系来解读链接和位置以及决定关节转换的基本程序。从参考点开始到第一个关节，再最终到最后一个关节完成转换后可以得到机械臂的转换矩阵。利用这个转换矩阵可以根据手臂末端的目标位置得到每个关节的转动参数。

我们主要采用Denavit Hartenberg (DH)来进行运动学的计算。根据厂家提供的机器人的手臂参数（如长度等），得到每个关节n的转换矩阵$A_n$

\begin{equation}
{{A}_n} = \left[ {\begin{array}{*{20}{c}}
	{{C_{{\theta _n}}}}&{ - {S_{{\theta _n}}}{C_{{\alpha _n}}}}&{{S_{{\theta _n}}}{S_{{\alpha _n}}}}&{{a_n}{C_{{\theta _n}}}}\\
	{{S_{{\theta _n}}}}&{{C_{{\theta _n}}}{C_{{\alpha _n}}}}&{ - {C_{{\theta _n}}}{S_{{\alpha _n}}}}&{{a_n}{S_{{\theta _n}}}}\\
	0&{{S_{{\alpha _n}}}}&{{C_{{\alpha _n}}}}&{{d_n}}\\
	0&0&0&1
	\end{array}} \right]
\end{equation}

其中${C_{{\theta _n}}} = \cos ({\theta _n})$,${S_{{\theta _n}}} = \sin ({\theta _n})$, ${{\theta }_{n}}$为官方给出的旋转角，$a_n$为两个关节参考坐标系中坐标轴交界口的距离，$d_n$为两个关节在同意旋转轴上的距离，这些数据都可以在NAO的数据手册中查询到。

为了根据目的点的坐标分离方程，以得到各个关节的转动值，我们将变换矩阵"An"的逆矩阵相乘于关系方程的左边来取得计算角度值的元素。右手包含五个关节，从上到下分别是肩膀关节(RShoulderPitch，RShoulderRoll 两个)，手肘关节(elbow RElbowRoll，RElbowYaw 两个)，和手腕关节(RWristYaw)。因此，表示从肩膀到手腕各关节的转动矩阵为${}_{S}^{W}{T}$

\begin{equation}
{}_{s}^{\rm{w}}{T = }{{A}_{1}} \times {{A}_{2}} \times {{A}_{3}} \times {{A}_{4}} \times {{A}_{5}}
\end{equation}

\begin{equation}
	A = {A_1} \times {A_2} \times {A_3} \times {A_4} \times {A_5} = \left[ {\begin{array}{*{20}{c}}
		{{r_{11}}}&{{r_{12}}}&{{r_{13}}}&{{p_x}}\\
		{{r_{21}}}&{{r_{22}}}&{{r_{23}}}&{{p_y}}\\
		{{r_{31}}}&{{r_{32}}}&{{r_{33}}}&{{p_z}}\\
		0&0&0&1
		\end{array}} \right]
\end{equation}

然后我们通过逐步乘以 $ A_i^{-1} $ 的方式得到各个关节的变换矩阵A，进而可以通过以下公式得到每个关节的转动参数。

\begin{equation}
{Pos\_w = A} \times {Pos\_s}
\end{equation}

由以上方程并结合NAO的链接关节变量数据（在NAO官网上提供），我们可以根据目的点得到每个关节点旋转的角度，从而达到基于逆运动学控制机器人的机械臂。



\chapter{项目展示}
下面以时间顺序分阶段介绍我们的成果：

\section{实时图像采集与显示系统}
实现了多自由度机械结构托举的双目摄像头，随着操作者头部的转动而同步转动，以采集实时图像信息。在这一阶段，我们的VR平台暂时使用使用谷歌折叠式纸板虚拟现实显示器。操作者头部姿态数据由VR头盔中的Android手机自身的陀螺仪获取，并发送到舵机的控制端。

同时，在这一阶段，我们在机械臂末端加上了吸盘执行机构，使用气泵和电磁阀组建了一个吸附装置(见图~\ref{bump})。


\begin{figure}
\begin{minipage}{0.48\textwidth}
  \centering
  \includegraphics[height=6cm]{car.png}
  \caption{全向轮底盘}
  \label{car}
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
  \centering
  \includegraphics[height=6cm]{bump.png}
  \caption{气泵和电磁阀组建的吸附装置}
  \label{bump}
\end{minipage}
\end{figure}


同时我们改写了DOBOT机械臂底层及接口，解决了串口控制卡顿的问题。实现用遥控器-接收机控制系统，远程完成叠纸杯实验。

\section{机械臂遥控与执行机构（Vive+DOBOT机械臂）}

在这一阶段，我们完成了一个由全向轮底盘移动的机械臂(见图~\ref{car})，平台上搭载多自由度机械结构托举的双目摄像头，随着操作者头部的转动而同步转动，以采集实时图像信息。

我们可以实现读取追踪器和VR头盔绝对位置并通过串口发送。可以看到图~(\ref{POS})中输出了左手，右手和头盔的绝对坐标。我们改写了机械臂底层及接口，解决了串口控制卡顿的情况。
\begin{figure}[htbp]
\small
\centering
\includegraphics[width=16cm]{Date.png}
\caption{追踪器和VR头盔绝对位置} 
\label{POS}
\end{figure}

从而实现了用Vive追踪器远程控制机械臂，完成了叠纸杯实验。

\section{机械臂遥控与执行机构（NAO人形机器人）}
另外我们将此系统迁移到NAO人形机器人上，用追踪器控制机器人手臂的运动。\footnote{由于NAO自身的缺陷（易失去平衡），我们现阶段并没有加入腿部动作，且将手臂动作对称处理}

成功完成Vive远程控制NAO的实验。

\section{总结}

经测试，我们这套系统远程控制实时性很好，视频传输感受不到延迟，在VR眼睛中有很强的立体感，控制很方便，追踪器定位效果很好，而且具有很强的可迁移性。



\chapter{展望}

\section{项目改进}

给NAO编写行走程序

通过采集每次操作的数据，可以使用增强学习训练机器人自主地执行任务。

为了完善机器人的功能，可以加上手势识别元件实现机器人手部的动作。


\section{未来应用}
此技术可以用于拆弹，救援，远程交互等领域。

随着科技技术的不断发展，人类正向许多未知的领域前进，而这个过程注定危险重重。如深水、深地层、外层空间、强放射、高真空等新的生产和作业领域。

例如，随着航天事业的不断发展，如今人类已经不再局限于把飞行器送上天，还要求它们完成各种空间任务。在地球上尚且需要制造机器人来代替人完成各种工作，而在太空中只有几个宇航员，在这样危险的环境中，就只需采用本技术的Vive，就可以在空间舱中看到外面，进而遥控机器人协助或代替宇航员完成大量艰巨危险的任务。

而且，面对高强度工作，可通过本技术遥控机器人代替完成。比如，可解决大型装备的安装、拆卸等工作。
 
未来此技术可以用于拆弹，救援，远程交互等领域。